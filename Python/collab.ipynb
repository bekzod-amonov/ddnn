{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optuna\n",
    "!git clone https://github.com/bekzod-amonov/ddnn-execute-once.git\n",
    "%cd ddnn-execute-once\n",
    "!ls\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.distributions import Normal, StudentT\n",
    "\n",
    "# -----------------------------\n",
    "# CUDA / device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "distribution = \"Normal\"  # change accordingly\n",
    "paramcount = {\n",
    "    \"Normal\": 2,\n",
    "    \"StudentT\": 3,\n",
    "    \"JSU\": 4,\n",
    "    \"SinhArcsinh\": 4,\n",
    "    \"NormalInverseGaussian\": 4,\n",
    "    \"Point\": None\n",
    "}\n",
    "if distribution not in paramcount:\n",
    "    raise ValueError(\"Incorrect distribution defined\")\n",
    "\n",
    "retrain_no = 13\n",
    "INP_SIZE   = 221\n",
    "bzn        = \"AT\"\n",
    "\n",
    "activations  = ['sigmoid', 'relu', 'elu', 'leaky_relu', 'tanh', 'softplus']\n",
    "_ACTS = {\n",
    "    'sigmoid':    nn.Sigmoid,\n",
    "    'relu':       nn.ReLU,\n",
    "    'elu':        nn.ELU,\n",
    "    'leaky_relu': nn.LeakyReLU,\n",
    "    'tanh':       nn.Tanh,\n",
    "    'softplus':   nn.Softplus,\n",
    "    'softmax':    nn.Softmax\n",
    "}\n",
    "\n",
    "START_YEAR      = 2020\n",
    "VAL_START_YEAR  = 2022\n",
    "TRAIN_END_YEAR  = 2023\n",
    "FINAL_END_YEAR  = 2025\n",
    "\n",
    "INIT_DATE_EXP   = pd.Timestamp(f'{START_YEAR}-01-01 00:00:00')\n",
    "VAL_INIT_DATE   = pd.Timestamp(f'{VAL_START_YEAR}-12-28 00:00:00')\n",
    "TRAIN_END_DATE  = pd.Timestamp(f'{TRAIN_END_YEAR}-12-27 00:00:00')\n",
    "FINAL_DATE_EXP  = pd.Timestamp(f\"{FINAL_END_YEAR}-12-31 00:00:00\")\n",
    "\n",
    "train_val_days = (TRAIN_END_DATE - INIT_DATE_EXP).days\n",
    "train_days     = (VAL_INIT_DATE - INIT_DATE_EXP).days\n",
    "val_days       = (TRAIN_END_DATE - VAL_INIT_DATE).days\n",
    "\n",
    "# -----------------------------\n",
    "# Repo root (NOTEBOOK SAFE)\n",
    "# -----------------------------\n",
    "# If your notebook is opened inside the repo folder, this is correct.\n",
    "# Otherwise set explicitly, e.g.: repo_root = Path(\"/content/<your_repo>\")\n",
    "repo_root = Path.cwd()\n",
    "\n",
    "trial_dir = repo_root / \"trialfiles\"\n",
    "trial_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(repo_root / f\"forecasts_probNN_{distribution.lower()}\").mkdir(exist_ok=True)\n",
    "(repo_root / f\"distparams_probNN_{distribution.lower()}\").mkdir(exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Load data\n",
    "# -----------------------------\n",
    "data_path = repo_root / \"Datasets\" / f\"{bzn}.csv\"\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing dataset: {data_path}\")\n",
    "\n",
    "data = pd.read_csv(data_path, index_col=0)\n",
    "data.index = [datetime.strptime(e, \"%Y-%m-%d %H:%M:%S\") for e in data.index]\n",
    "data = data.sort_index()\n",
    "\n",
    "# -----------------------------\n",
    "# Model\n",
    "# -----------------------------\n",
    "class ProbMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        widths: Tuple[int, int],           # (neurons_1, neurons_2)\n",
    "        activations: Tuple[str, str],\n",
    "        output_dim: int,\n",
    "        use_dropout: bool,\n",
    "        dropout_p: float,\n",
    "        distribution: str,\n",
    "        return_hidden: bool = True,\n",
    "        use_batchnorm: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        w1, w2 = widths\n",
    "        a1, a2 = activations\n",
    "\n",
    "        self.distribution = distribution\n",
    "        self.return_hidden = return_hidden\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(input_dim) if use_batchnorm else None\n",
    "        self.dropout = nn.Dropout(dropout_p) if use_dropout else None\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, w1)\n",
    "        self.act1 = _ACTS[a1.lower()]()\n",
    "\n",
    "        self.fc2 = nn.Linear(w1, w2)\n",
    "        self.act2 = _ACTS[a2.lower()]()\n",
    "\n",
    "        # Heads\n",
    "        if distribution == \"Point\":\n",
    "            self.head = nn.Linear(w2, output_dim)\n",
    "\n",
    "        elif distribution == \"Normal\":\n",
    "            self.head_loc   = nn.Linear(w2, output_dim)\n",
    "            self.head_scale = nn.Linear(w2, output_dim)\n",
    "\n",
    "        elif distribution == \"StudentT\":\n",
    "            self.head_loc   = nn.Linear(w2, output_dim)\n",
    "            self.head_scale = nn.Linear(w2, output_dim)\n",
    "            self.head_df    = nn.Linear(w2, output_dim)\n",
    "\n",
    "        elif distribution in (\"JSU\", \"SinhArcsinh\", \"NormalInverseGaussian\"):\n",
    "            # placeholder heads (you said you will add families later)\n",
    "            self.head_loc        = nn.Linear(w2, output_dim)\n",
    "            self.head_scale      = nn.Linear(w2, output_dim)\n",
    "            self.head_tailweight = nn.Linear(w2, output_dim)\n",
    "            self.head_skewness   = nn.Linear(w2, output_dim)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"unsupported distribution: {distribution}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        h1 = self.act1(self.fc1(x))\n",
    "        if self.dropout is not None:\n",
    "            h1 = self.dropout(h1)\n",
    "\n",
    "        h2 = self.act2(self.fc2(h1))\n",
    "        if self.dropout is not None:\n",
    "            h2 = self.dropout(h2)\n",
    "\n",
    "        if self.distribution == \"Point\":\n",
    "            y = self.head(h2)\n",
    "            return (y, (h1, h2)) if self.return_hidden else y\n",
    "\n",
    "        if self.distribution == \"Normal\":\n",
    "            loc   = self.head_loc(h2)\n",
    "            scale = 1e-3 + 3.0 * F.softplus(self.head_scale(h2))\n",
    "            params = {\"loc\": loc, \"scale\": scale}\n",
    "            return (params, (h1, h2)) if self.return_hidden else params\n",
    "\n",
    "        if self.distribution == \"StudentT\":\n",
    "            loc   = self.head_loc(h2)\n",
    "            scale = 1e-3 + 3.0 * F.softplus(self.head_scale(h2))\n",
    "            df    = 1.0 + 3.0 * F.softplus(self.head_df(h2))\n",
    "            params = {\"loc\": loc, \"scale\": scale, \"df\": df}\n",
    "            return (params, (h1, h2)) if self.return_hidden else params\n",
    "\n",
    "        if self.distribution in (\"JSU\", \"SinhArcsinh\", \"NormalInverseGaussian\"):\n",
    "            loc   = self.head_loc(h2)\n",
    "            scale = 1e-3 + 3.0 * F.softplus(self.head_scale(h2))\n",
    "            tailweight = 1.0 + 3.0 * F.softplus(self.head_tailweight(h2))\n",
    "            skewness   = self.head_skewness(h2)\n",
    "            params = {\"loc\": loc, \"scale\": scale, \"tailweight\": tailweight, \"skewness\": skewness}\n",
    "            return (params, (h1, h2)) if self.return_hidden else params\n",
    "\n",
    "        raise RuntimeError(\"distribution branch fell through\")\n",
    "\n",
    "    def make_dist(self, params):\n",
    "        if self.distribution == \"Normal\":\n",
    "            return Normal(loc=params[\"loc\"], scale=params[\"scale\"])\n",
    "        if self.distribution == \"StudentT\":\n",
    "            return StudentT(df=params[\"df\"], loc=params[\"loc\"], scale=params[\"scale\"])\n",
    "        # placeholders for later\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# Global for rolling window\n",
    "# -----------------------------\n",
    "# Only GPU-side change requested: increase batch size.\n",
    "# (All CPU feature construction is left exactly as-is.)\n",
    "BATCH_SIZE = 2048  # <-- ONLY GPU-side change; feel free to set 4096/8192 if it fits.\n",
    "\n",
    "PIN_MEMORY = bool(torch.cuda.is_available())\n",
    "\n",
    "def _param_names_for_distribution(dist: str):\n",
    "    if dist == \"Normal\":\n",
    "        return [\"loc\", \"scale\"]\n",
    "    if dist == \"StudentT\":\n",
    "        return [\"loc\", \"scale\", \"df\"]\n",
    "    if dist in (\"JSU\", \"SinhArcsinh\", \"NormalInverseGaussian\"):\n",
    "        return [\"loc\", \"scale\", \"tailweight\", \"skewness\"]\n",
    "    return []\n",
    "\n",
    "def rolling_window(inp):\n",
    "    best_params, day_no = inp\n",
    "\n",
    "    start = data.index.searchsorted(INIT_DATE_EXP) + day_no * 24\n",
    "    df_train_val = data.iloc[start : start + train_val_days * 24 + 24]\n",
    "    Y_train_val  = np.zeros((train_val_days, 24))\n",
    "\n",
    "    train_start = df_train_val.index[0]\n",
    "    train_end   = df_train_val.index[train_val_days * 24 - 1]\n",
    "    pred_day    = df_train_val.index[train_val_days * 24]\n",
    "\n",
    "    total_rolls = (end - base) // 24 - train_val_days\n",
    "    if day_no % 25 == 0 or day_no == total_rolls - 1:\n",
    "        print(f\"[{day_no+1}/{total_rolls}] train: {train_start:%Y-%m-%d} -> {train_end:%Y-%m-%d %H:%M} | predict day: {pred_day:%Y-%m-%d}\")\n",
    "\n",
    "    for d in range(Y_train_val.shape[0]):\n",
    "        Y_train_val[d, :] = df_train_val.loc[df_train_val.index[d*24:(d+1)*24], \"Price\"].to_numpy()\n",
    "    Y_train_val = Y_train_val[7:, :]  # skip first 7 days due to lagged features\n",
    "\n",
    "    X_train_val = np.zeros((train_val_days + 1, INP_SIZE))\n",
    "    for d in range(7, X_train_val.shape[0]):\n",
    "        X_train_val[d, :24]      = df_train_val.loc[df_train_val.index[(d-1)*24:(d*24)], \"Price\"].to_numpy()\n",
    "        X_train_val[d, 24:48]    = df_train_val.loc[df_train_val.index[(d-2)*24:((d-1)*24)], \"Price\"].to_numpy()\n",
    "        X_train_val[d, 48:72]    = df_train_val.loc[df_train_val.index[((d-3)*24):((d-2)*24)], \"Price\"].to_numpy()\n",
    "        X_train_val[d, 72:96]    = df_train_val.loc[df_train_val.index[((d-7)*24):((d-6)*24)], \"Price\"].to_numpy()\n",
    "        X_train_val[d, 96:120]   = df_train_val.loc[df_train_val.index[(d*24):((d+1)*24)], \"Load_DA\"].to_numpy()\n",
    "        X_train_val[d, 120:144]  = df_train_val.loc[df_train_val.index[((d-1)*24):(d*24)], \"Load_DA\"].to_numpy()\n",
    "        X_train_val[d, 144:168]  = df_train_val.loc[df_train_val.index[((d-7)*24):((d-6)*24)], \"Load_DA\"].to_numpy()\n",
    "        X_train_val[d, 168:192]  = df_train_val.loc[df_train_val.index[(d*24):((d+1)*24)], \"Renewables_DA_Forecast\"].to_numpy()\n",
    "        X_train_val[d, 192:216]  = df_train_val.loc[df_train_val.index[((d-1)*24):(d*24)], \"Renewables_DA_Forecast\"].to_numpy()\n",
    "        X_train_val[d, 216]      = df_train_val.loc[df_train_val.index[(d( d-2)*24):(d-1)*24:24], \"EUA\"].to_numpy().item()\n",
    "\n",
    "        X_train_val[d, 216] = df_train_val.loc[df_train_val.index[(d-2)*24:(d-1)*24:24], \"EUA\"].to_numpy().item()\n",
    "        X_train_val[d, 217] = df_train_val.loc[df_train_val.index[(d-2)*24:(d-1)*24:24], \"Coal\"].to_numpy().item()\n",
    "        X_train_val[d, 218] = df_train_val.loc[df_train_val.index[(d-2)*24:(d-1)*24:24], \"NGas\"].to_numpy().item()\n",
    "        X_train_val[d, 219] = df_train_val.loc[df_train_val.index[(d-2)*24:(d-1)*24:24], \"Oil\"].to_numpy().item()\n",
    "        X_train_val[d, 220] = df_train_val.index[d].weekday()\n",
    "\n",
    "    # feature selection\n",
    "    colmask = [False] * INP_SIZE\n",
    "    if best_params['price_D-1']: colmask[:24]     = [True] * 24\n",
    "    if best_params['price_D-2']: colmask[24:48]   = [True] * 24\n",
    "    if best_params['price_D-3']: colmask[48:72]   = [True] * 24\n",
    "    if best_params['price_D-7']: colmask[72:96]   = [True] * 24\n",
    "    if best_params['load_DA']:   colmask[96:120]  = [True] * 24\n",
    "    if best_params['load_DA_D-1']: colmask[120:144] = [True] * 24\n",
    "    if best_params['load_DA_D-7']: colmask[144:168] = [True] * 24\n",
    "    if best_params['RES_DA_D']: colmask[168:192]  = [True] * 24\n",
    "    if best_params['RES_DA_D-1']: colmask[192:216] = [True] * 24\n",
    "    if best_params['EUA']:  colmask[216] = True\n",
    "    if best_params['Coal']: colmask[217] = True\n",
    "    if best_params['NGas']: colmask[218] = True\n",
    "    if best_params['Oil']:  colmask[219] = True\n",
    "    if best_params['Week_Day_Dummy']: colmask[220] = True\n",
    "\n",
    "    X_train_val = X_train_val[:, colmask]\n",
    "\n",
    "    X_predict   = X_train_val[-1:, :]\n",
    "    X_train_val = X_train_val[7:-1, :]\n",
    "\n",
    "    widths = (best_params[\"neurons_1\"], best_params[\"neurons_2\"])\n",
    "    activation_function = (best_params['activation_1'], best_params['activation_2'])\n",
    "\n",
    "    use_batchnorm = True\n",
    "    return_hidden = True\n",
    "    output_dim    = 24\n",
    "\n",
    "    use_dropout = best_params['dropout']\n",
    "    dropout_p = best_params['dropout_p'] if use_dropout else 0.0\n",
    "\n",
    "    regularize_h1_activation = best_params['regularize_h1_activation']\n",
    "    h1_activation_rate = 0.0 if not regularize_h1_activation else best_params['h1_activation_rate_l1']\n",
    "\n",
    "    regularize_h1_kernel = best_params['regularize_h1_kernel']\n",
    "    h1_kernel_rate = 0.0 if not regularize_h1_kernel else best_params['h1_kernel_rate_l1']\n",
    "\n",
    "    regularize_h2_activation = best_params['regularize_h2_activation']\n",
    "    h2_activation_rate = 0.0 if not regularize_h2_activation else best_params['h2_activation_rate_l1']\n",
    "\n",
    "    regularize_h2_kernel = best_params['regularize_h2_kernel']\n",
    "    h2_kernel_rate = 0.0 if not regularize_h2_kernel else best_params['h2_kernel_rate_l1']\n",
    "\n",
    "    # head L1 rates: use correct names per distribution\n",
    "    head_l1_rates: Dict[str, float] = {}\n",
    "    param_names = _param_names_for_distribution(distribution)\n",
    "    if paramcount[distribution] is not None:\n",
    "        for name in param_names:\n",
    "            reg_flag = best_params.get(f\"regularize_{name}\", False)\n",
    "            rate = 0.0 if not reg_flag else best_params.get(f\"{name}_rate_l1\", 0.0)\n",
    "            head_l1_rates[name] = rate\n",
    "\n",
    "    learning_rate = best_params['learning_rate']\n",
    "    epochs = best_params['epochs']\n",
    "\n",
    "    # simple random split inside each rolling window (kept as you wrote it)\n",
    "    VAL_DATA = 0.2\n",
    "    N = X_train_val.shape[0]\n",
    "    perm = np.random.permutation(N)\n",
    "    cut = int((1.0 - VAL_DATA) * N)\n",
    "    train_idx = perm[:cut]\n",
    "    val_idx   = perm[cut:]\n",
    "\n",
    "    X_t = torch.as_tensor(X_train_val, dtype=torch.float32)\n",
    "    Y_t = torch.as_tensor(Y_train_val, dtype=torch.float32)\n",
    "\n",
    "    full_ds = TensorDataset(X_t, Y_t)\n",
    "    train_ds = torch.utils.data.Subset(full_ds, train_idx.tolist())\n",
    "    val_ds   = torch.utils.data.Subset(full_ds, val_idx.tolist())\n",
    "\n",
    "    # ONLY GPU-SIDE CHANGE: bigger batch size; plus pin_memory/non_blocking for CUDA\n",
    "    bs = min(BATCH_SIZE, len(train_ds)) if len(train_ds) > 0 else 1\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True,  pin_memory=PIN_MEMORY, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=bs, shuffle=False, pin_memory=PIN_MEMORY, num_workers=0)\n",
    "\n",
    "    model = ProbMLP(\n",
    "        input_dim     = X_t.shape[1],\n",
    "        widths        = widths,\n",
    "        activations   = activation_function,\n",
    "        output_dim    = output_dim,\n",
    "        use_batchnorm = use_batchnorm,\n",
    "        use_dropout   = use_dropout,\n",
    "        dropout_p     = dropout_p,\n",
    "        return_hidden = return_hidden,\n",
    "        distribution  = distribution,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "    mae = nn.L1Loss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            params_pred, (h1, h2) = model(x)\n",
    "\n",
    "            if distribution == 'Point':\n",
    "                loss_train = mae(params_pred, y)\n",
    "            elif distribution == 'Normal':\n",
    "                dist = Normal(loc=params_pred[\"loc\"], scale=params_pred[\"scale\"])\n",
    "                loss_train = (-dist.log_prob(y)).mean()\n",
    "            elif distribution == 'StudentT':\n",
    "                dist = StudentT(df=params_pred[\"df\"], loc=params_pred[\"loc\"], scale=params_pred[\"scale\"])\n",
    "                loss_train = (-dist.log_prob(y)).mean()\n",
    "            else:\n",
    "                raise ValueError(\"Implement training loss for this distribution\")\n",
    "\n",
    "            # hidden kernel L1\n",
    "            weight_l1 = torch.tensor(0.0, device=device)\n",
    "            if regularize_h1_kernel:\n",
    "                weight_l1 = weight_l1 + h1_kernel_rate * model.fc1.weight.abs().sum()\n",
    "            if regularize_h2_kernel:\n",
    "                weight_l1 = weight_l1 + h2_kernel_rate * model.fc2.weight.abs().sum()\n",
    "\n",
    "            # activation L1\n",
    "            act_l1 = torch.tensor(0.0, device=device)\n",
    "            if regularize_h1_activation:\n",
    "                act_l1 = act_l1 + h1_activation_rate * h1.abs().sum()\n",
    "            if regularize_h2_activation:\n",
    "                act_l1 = act_l1 + h2_activation_rate * h2.abs().sum()\n",
    "\n",
    "            # head L1\n",
    "            head_l1 = torch.tensor(0.0, device=device)\n",
    "            head_modules = {\n",
    "                'loc': getattr(model, 'head_loc', None),\n",
    "                'scale': getattr(model, 'head_scale', None),\n",
    "                'df': getattr(model, 'head_df', None),\n",
    "                'tailweight': getattr(model, 'head_tailweight', None),\n",
    "                'skewness': getattr(model, 'head_skewness', None),\n",
    "            }\n",
    "            for name, rate in head_l1_rates.items():\n",
    "                m = head_modules.get(name, None)\n",
    "                if (m is not None) and (rate > 0.0):\n",
    "                    head_l1 = head_l1 + rate * m.weight.abs().sum()\n",
    "\n",
    "            loss_reg = loss_train + weight_l1 + act_l1 + head_l1\n",
    "            loss_reg.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Xf_t = torch.as_tensor(X_predict, dtype=torch.float32).to(device, non_blocking=True)\n",
    "        params_pred, _ = model(Xf_t)\n",
    "\n",
    "        # export dist params\n",
    "        if distribution == \"Normal\":\n",
    "            getters = {\"loc\": params_pred[\"loc\"], \"scale\": params_pred[\"scale\"]}\n",
    "        elif distribution == \"StudentT\":\n",
    "            getters = {\"loc\": params_pred[\"loc\"], \"scale\": params_pred[\"scale\"], \"df\": params_pred[\"df\"]}\n",
    "        else:\n",
    "            raise ValueError(\"Implement getters for this distribution\")\n",
    "\n",
    "        params_out = {k: [float(e) for e in v.detach().cpu().numpy()[0]] for k, v in getters.items()}\n",
    "        day_key = datetime.strftime(df_train_val.index[-24], \"%Y-%m-%d\")\n",
    "\n",
    "        with open(repo_root / f\"distparams_probNN_{distribution.lower()}\" / f\"{day_key}.json\", \"w\") as f:\n",
    "            json.dump(params_out, f)\n",
    "\n",
    "        # scenarios + mean forecast\n",
    "        if distribution == \"Normal\":\n",
    "            dist = Normal(loc=getters[\"loc\"], scale=getters[\"scale\"])\n",
    "        else:\n",
    "            dist = StudentT(df=getters[\"df\"], loc=getters[\"loc\"], scale=getters[\"scale\"])\n",
    "\n",
    "        pred = dist.sample((10000,)).squeeze(1).detach().cpu().numpy()\n",
    "        pred_mean = pred.mean(axis=0)\n",
    "\n",
    "        np.savetxt(\n",
    "            repo_root / f\"forecasts_probNN_{distribution.lower()}\" / f\"{day_key}.csv\",\n",
    "            pred, delimiter=\",\", fmt=\"%.3f\"\n",
    "        )\n",
    "\n",
    "        predDF = pd.DataFrame(index=df_train_val.index[-24:])\n",
    "        predDF[\"real\"] = df_train_val.loc[df_train_val.index[-24:], \"Price\"].to_numpy()\n",
    "        predDF[\"forecast\"] = pred_mean\n",
    "\n",
    "    # we do not return anything to avoid building a huge list in RAM\n",
    "    # (files are already written)\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Load Optuna study (sqlite file in repo)\n",
    "# -----------------------------\n",
    "study_name = f'FINAL_{bzn}_selection_prob_{distribution.lower()}'\n",
    "db_path = (repo_root / \"trialfiles\" / f\"{study_name}.db\").resolve()\n",
    "storage_name = f\"sqlite:///{db_path.as_posix()}\"\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True)\n",
    "print(study.trials_dataframe().tail())\n",
    "best_params = study.best_params\n",
    "print(\"best_params:\", best_params)\n",
    "\n",
    "# -----------------------------\n",
    "# Rolling evaluation (NO multiprocessing; notebook-safe)\n",
    "# -----------------------------\n",
    "base = data.index.searchsorted(INIT_DATE_EXP)\n",
    "end  = data.index.searchsorted(FINAL_DATE_EXP)\n",
    "total_days = (end - base) // 24\n",
    "\n",
    "total_rolls = total_days - train_val_days\n",
    "print(\"total_rolls:\", total_rolls)\n",
    "\n",
    "inputlist = [(best_params, day) for day in range(total_rolls)]\n",
    "print(\"len(inputlist):\", len(inputlist))\n",
    "\n",
    "print(\"==================== Step 2 - NN evaluation ====================\")\n",
    "for args in inputlist:\n",
    "    rolling_window(args)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
